[
  {
    "objectID": "slides.html#interactive-plotly",
    "href": "slides.html#interactive-plotly",
    "title": "PicoCal Autoencoder",
    "section": "Interactive Plotly",
    "text": "Interactive Plotly"
  },
  {
    "objectID": "slides.html#what-i-am-going-to-do",
    "href": "slides.html#what-i-am-going-to-do",
    "title": "PicoCal Autoencoder",
    "section": "What I am going to do",
    "text": "What I am going to do\nOptimize the model via quantization, focusing on the encoder."
  },
  {
    "objectID": "slides.html#wrong-baseline-comparison-fixed",
    "href": "slides.html#wrong-baseline-comparison-fixed",
    "title": "PicoCal Autoencoder",
    "section": "Wrong baseline comparison (fixed)",
    "text": "Wrong baseline comparison (fixed)\nInitially, the quantized model was asked to reconstruct unquantized inputs; this implicitly asks it to “undo” input quantization too.\nTODO: show the previous code for quantized model and focus on the line with activation before the model\nFix: Quantize the inputs (data) prior to training so both models see consistent quantization."
  },
  {
    "objectID": "slides.html#dying-relu-diagnosis-fixed",
    "href": "slides.html#dying-relu-diagnosis-fixed",
    "title": "PicoCal Autoencoder",
    "section": "“dying ReLU” diagnosis (fixed)",
    "text": "“dying ReLU” diagnosis (fixed)\nIntermittent failures (~1/3 runs).\nEven after fixing the inputs, about one-third of runs failed to learn. Inspection of weights and biases showed nothing obviously wrong, yet parameter updates were tiny or stagnant across batches/epochs. Probing intermediate layer outputs revealed the network was producing near-constant outputs, and, crucially, the encoder ReLU had a very high (sometimes ~100%) probability of outputting zero on one or both latent units."
  }
]