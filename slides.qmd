---
title: "PicoCal Autoencoder"
author: "Ali Javani"
date: "03.09.2025"
bibliography: references.bib
csl: assets/ieee.csl
format:
  revealjs:
    navigation-mode: vertical   
    theme: dracula
    slide-number: true
    progress: true
    hash: true
    transition: slide
    controls: true
    preview-links: auto
    center: true
    footer: "Ali Javani - Summer Student 2025 - PicoCal Autoencoder Optimization - 03.09.2025"
    logo: "assets/images/logo/cern-white.png"
    title-slide-attributes:
      data-visibility: "hidden"
    # fontsize: 1%
execute:
  echo: false        # hide code by default (nice for slides)
  warning: false
jupyter: python3     # optional; Quarto will pick Jupyter automatically
---
#

<div style="text-align:center; margin-bottom:1.2rem;">
  <h1 style="font-size: 2.3em;">PicoCal Autoencoder Optimization</h1>
  <p><strong>Presenter:</strong> Ali Javani</p>
  <p><strong>Supervisers:</strong> Dr. Vladimir Loncar • Prof. Eluned Smith</p>
  <p style="font-size: .6em;"><strong>Co-Supervisors:</strong> Dr. Julian Garcia Pardinas • Dr. Katya Govorkova</p>
</div>

<div style="display:flex; justify-content:center; gap:20px">
  <div><img src="assets/images/logo/psl.svg" alt="Main University" style="height:70px; filter: brightness(1.5)"></div>
  <div><img src="assets/images/logo/cern.png" alt="Another Affiliation" style="height:70px; brightness(1.5)"></div>
  <div><img src="assets/images/logo/sft.png" alt="Institute of Advanced Things" style="height:70px; brightness(1.5)"></div>
  <div><img src="assets/images/logo/mit.png" alt="Another Affiliation" style="height:70px; brightness(1.5)"></div>
</div>

<!-- Set Drakual Theme for Plotly -->

```{python}
import plotly.io as pio
import plotly.graph_objects as go

# Dracula palette (official-ish colors)
DRACULA_BG = "#2d2f39"
DRACULA_GRID = "#44475a"
DRACULA_TEXT = "#f8f8f2"
DRACULA_COLORWAY = [
    "#8be9fd",  # cyan
    "#50fa7b",  # green
    "#ffb86c",  # orange
    "#ff79c6",  # pink
    "#bd93f9",  # purple
    "#ff5555",  # red
    "#f1fa8c",  # yellow
]

dracula_layout = dict(
    colorway=DRACULA_COLORWAY,
    paper_bgcolor=DRACULA_BG,
    plot_bgcolor=DRACULA_BG,
    font=dict(color=DRACULA_TEXT),
    title=dict(x=0.05),
    xaxis=dict(
        gridcolor=DRACULA_GRID, zerolinecolor=DRACULA_GRID,
        linecolor=DRACULA_GRID, automargin=True
    ),
    yaxis=dict(
        gridcolor=DRACULA_GRID, zerolinecolor=DRACULA_GRID,
        linecolor=DRACULA_GRID, automargin=True
    ),
    legend=dict(bgcolor="rgba(0,0,0,0)"),
)

pio.templates["dracula"] = go.layout.Template(layout=dracula_layout)
pio.templates.default = "dracula"
```

# Project overview

The model compresses and reconstructs the pulses measured in the **PicoCal** [@LHCbcollaboration:2903094] calorimeter. Each pulse is normally stored as **32 samples** (already downsampled from a longer waveform). Transmitting and saving all of this is challenging because the detector will record **millions of pulses per second**.

---

Our **autoencoder** takes each 32-point pulse and compresses it to a **2-dimensional latent**. These two numbers capture key features (size, shape, timing). The **encoder** runs **on-detector** so only the compressed representation is transmitted; the **decoder** reconstructs the full 32-sample pulse downstream.

---

Compared with simpler methods that keep only a timestamp, this preserves much more information about the **pulse shape**, improving:

- **Time reconstruction:** precise hit timing.
- **Energy calibration:** mapping the wave amplitude to deposited energy.
- **Pile-up mitigation:** separating overlapping signals.

# What is an Autoencoder?

An autoencoder learns a compact representation (**encoder**) and a reconstruction (**decoder**).

Our baseline: **32 → 2 → 32** (encoder → bottleneck → decoder). The encoder is the on-detector component; the decoder is offline.

So, the bottleneck would retain most of the information

---

![](assets/images/autoencoder.png)

# Data used for training

Training and testing data come from a detailed simulation of the PicoCal electromagnetic calorimeter, including the detector geometry, materials, and readout electronics, so simulated pulses resemble those expected during LHCb data-taking.

---

We generate two pulse types and then mix them to mimic realistic running conditions:

- **Signal pulses:** single photons, 0.5–5 GeV, produced near the interaction point.
- **Background pulses:** extra detector activity (for example: pile-up, underlying event).

Each pulse is first simulated with **1024 samples** (as in the actual test-beam digitizer). The detector will use a different digitizer with **32 samples**, so we **downsample** 1024→32 to match. This preserves overall shape but not exact timing.

# Current status (work done so far)

- A baseline autoencoder is **trained**.
- An **hls4ml** implementation exists for **hardware deployment** of the encoder.

## What I am going to do

Optimize the model via **quantization**, focusing on the **encoder**.

## What has been changed about my project
::: {.slide style="font-size:70%"}
Our planned radiation hardening study (separate project) has been postponed because:

- **Uncertainty**: The specific effects of radiation on the hardware are still under investigation, so we lack a clear fault model to design for.
- **Current Efficiency**: Our hardware implementation is so resource-efficient that traditional Triple Modular Redundancy (TMR) is now a simple and viable solution, making a complex study less urgent.

➡️ Therefore, our focus has shifted toward the model quantization project.
:::

# Why we should quantize?

::: {.slide style="font-size:95%"}
- **Smaller & cheaper:** fewer bits → less bandwidth per pulse.
- **Faster & lower latency:** fixed-point arithmetic is hardware-friendly.
- **Lower power:** critical for on-detector operation.
- **Smaller output payload:** compressed latent space uses fewer bits per value.
- **All previous benefits are free:** with **QAT** ^[Quantization Aware Training (explained in the next slide)], We can recover most or all of the accuracy lost to quantization.
:::

#

::: {.panel-tabset style="font-size:70%"}

### Quantization granularities

| Option                                 | Definition                                                                                                                   | Pros                                                                                    | Costs / Trade-offs                                                                         | Typical use                                                                                                                                                                                       |
|----------------------------------------|------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Homogeneous**                        | One bit-width for **all** weights & activations across the model (e.g., all INT8).                                           | Simple, fast to deploy; widely supported; easy to calibrate.                            | May leave accuracy on the table; no layer-specific tuning.                                 | First pass; production baselines; strict simplicity/latency targets.                                                                                                                              |
| **Mixed-precision (layer/group-wise)** | Choose bit-width **per layer** or **per group of layers**<br/>(e.g., encoder vs. decoder; early/mid/late blocks).            | Strong accuracy–resource trade with modest complexity;<br/>aligns with runtime support. | Requires profiling/tuning;<br/>slightly more complex conversion.                           | FPGAs/ASICs (e.g., QKeras+hls4ml);<br/>mobile/edge where certain layers bottleneck.                                                                                                               |
| **Mixed-precision (per-weight-wise)**  | Bit-width varies **within a layer**<br/>(per channel/filter or even per individual weight).                                  | Finest control; can hit tight resource/accuracy targets;<br/>great for LUT/DSP packing. | High implementation complexity; limited framework/kernel support;<br/>harder verification. | **Hardware compilation flows (FPGA/ASIC)**—like those commonly used at **CERN** for on-detector/trigger systems;<br/>bit-serial or weight-streaming architectures;<br/>research/ASIC prototyping. |


### Quantization approaches

| Approach          | What it does                                                     | Accuracy impact                                                 | Speed/Memory win                         | Typical use                            |
| ----------------- | ---------------------------------------------------------------- | --------------------------------------------------------------- | ---------------------------------------- | -------------------------------------- |
| **PTQ (static)**  | Calibrate once; bake fixed activation scales/zero-points.        | Low→moderate (strong at INT8; riskier ≤4-bit)                   | 3–4× smaller; fast INT kernels           | Quick deploy w/ small calibration set. |
| **PTQ (dynamic)** | Weights quantized; activation scales computed at inference.      | Similar to static at INT8; worse ≤4-bit                         | Memory win; modest speedup               | CPU/NLP, no calibration set.           |
| **QAT**           | Train with fake-quant so model learns quant noise.               | Best (often matches FP at INT8; workable ≤4-bit with care)      | Same runtime as PTQ; extra training cost | Edge/FPGA/ASIC; tight accuracy.        |
| **WOQ**           | Quantize weights only (e.g., 4-bit); activations FP8/FP16.       | Small→moderate                                                  | Big memory/BW savings                    | Large LLM/CV on GPUs.                  |


:::

## Quantization Strategy: Mixed-precision QAT

::: {.slide style="font-size:60%"}
- **Why QAT?**
    - Small model → can retrain multiple times
    - Avoids accuracy loss from post-training quantization
- **Granularity Choice**
    - Few parameters → finer granularity is better
    - **Ideal**: Weight-wise mixed-precision (HGQ2 - High Granularity Quantization 2)
    - **Current**: Layer-wise quantization (QKeras)
- **Implementation**
    - **Now**: QKeras + hls4ml (layer-wise)
        - Stable support for Microchip radiation-hardened FPGA
    - **Future**: Migrate to HGQ2 (weight-wise)
        - Microchip FPGA support recently added but not production-ready
:::

# QKeras [@Coelho2020UltraLL] + hls4ml [@fahim2021hls4mlopensourcecodesignworkflow] + ~~(AutoQKeras [@Coelho_2021])~~

**QKeras is** a drop-in extension of Keras that lets you assign **per-layer** (heterogeneous) quantization to weights, activations, and batch-norm parameters and train the model **quantization-aware**. Combined with **hls4ml**, quantized networks are compiled to FPGA firmware for **O(10) ns** inference.

**AutoQKeras**: Auto search over bit-widths based on estimated energy or model bit size + accuracy.


## Why I'm not using AutoQKeras

**AutoQKeras** (auto search over bit-widths based on estimated energy or model bit size + accuracy) was **not used** due to:

- Sparse docs/API; heavy implicit behavior.
- Minimal result summaries and limited reliability for our workflow.
- No built-in **K-Fold** support;
- Limited parallelism beyond default Keras options (except Trial parallelism, Data parallelism only suitable for multiple GPUs)

# Environment constraints (what currently works)

::: {.slide style="font-size:70%"}
- QKeras works reliably with **`Python 3.10` + `TensorFlow 2.14` + `Keras 2.x`** in our tests.
- **Keras v3** APIs are not compatible (errors encountered). There are signs of in-progress support, but it is not usable in our setup yet and updates are so rare.
- CUDA/driver combinations: in practice, the project failed with some versions (e.g., ~560, ~580) and ran with an intermediate one (~570) but again, in theory it should be compatible with 570 ≤.
- Although some guides suggest **`TF 2.15`** (`Keras 2.15`) or **`TF ≥ 2.16` with `tf-keras` and `TF_USE_LEGACY_KERAS=1`**, this did not work in our experiments. We therefore **stuck to `Python 3.10` + `TF 2.14`**.
:::

---

*Note:* I plan to create a short **notebook/markdown guide** for a reproducible QKeras setup (including CERN cluster tips), given the current documentation gaps and the fact that it took a lot of time on setting up a working environment for me.

# Quantization work so far {style="font-size:140%"}


## Wrong baseline comparison (fixed)

::: {.slide style="font-size:70%"}
Initially, the quantized model was asked to reconstruct **unquantized** labes from it's quantized inputs; this implicitly asks it to reverse quantization function too.
:::

```{.python code-line-numbers="|3-4"}
quant_size = 8, int_size = 3

input_layer = layers.Input(shape=(32,), name="input_layer")
quantized_input = QActivation(f'quantized_bits(10,3,0,alpha=1)')(input_layer)
encoded_output = QDense(
    2,
    kernel_quantizer=f'quantized_bits({quant_size},{int_size},1,alpha=1)',
    bias_quantizer=f'quantized_bits({quant_size},{int_size},1,alpha=1)',
    name="encoded"
)(input_layer)
encoded_output = QActivation(
    f'quantized_relu({quant_size},{int_size},0)', 
    name="encoded_act"
)(encoded_output)
```

::: {.slide style="font-size:70%"}
**Fix:** **Quantize the inputs** (data) prior to training so both models see consistent quantization.
:::

## “dying ReLU” diagnosis (fixed)

**Intermittent failures (~1/3 runs).**

Even after fixing the inputs, about **one-third of runs** failed to learn. Inspection of **weights and biases** showed nothing obviously wrong, yet parameter updates were **tiny or stagnant** across batches/epochs. Probing intermediate **layer outputs** revealed the network was producing **near-constant outputs**, and, crucially, the **encoder ReLU** had a **very high (sometimes ~100%) probability** of outputting **zero** on one or both latent units.

---

**Root cause: ReLU + tiny bottleneck → dead units.**

With an activation that has an **asymmetric output** (ReLU = zero for all negative pre-activations) and a **bottleneck with very few neurons (latent=2)**, the model often explores **zero outputs** early—before the optimizer is “calibrated.” Once a ReLU unit is stuck at zero, its **gradient is zero** and it cannot recover. This is the **dying ReLU** problem: ReLU neurons become permanently inactive, outputting zero for all inputs and **blocking learning**.

---

**Mitigations explored (and what actually worked).**

::: {.slide style="font-size:70%"}
- **Initialization:** Use **He (Kaiming) initialization** instead of Xavier for rectifiers. [@he2015delvingdeeprectifierssurpassing]
- **Normalization:** Some papers suggest placing **Batch Normalization immediately before the activation** (notably in larger models) to stabilize pre-activations. [@DBLP:journals/corr/IoffeS15]
- **Activation variants:** Many works propose **ReLU extensions** (e.g., **Leaky ReLU / PReLU / RReLU**) that keep a **non-zero slope** on the negative side, both **mitigating dead units** and sometimes **improving accuracy**.
- **Regularization for quantization robustness:** One paper recommends **L2 regularization** in quantization-friendly designs for large models—shrinking weight ranges to **reduce quantization error**, which in turn lowers the chance of saturating activations and **killing neurons**: [@Sheng_2018]
- **Learning rate:** A **large LR** increases the chance of pushing units into the zero region. I **tuned the LR**, added **warmup**, and experimented with **restarts**; these helped but **did not** fully resolve the issue.
:::

---

**Outcome.**

The **only change that consistently fixed the problem** was **replacing ReLU with Leaky ReLU** in the encoder bottleneck. Other mitigations (He init, BN placement, L2, LR schedules) **did not** noticeably alleviate the issue in our **small** model, likely because many of those results are demonstrated on **larger architectures**.

---

**Operational safeguard.**

I also implemented a **callback** to detect dying behavior (monitoring **zero-output ratio** and **learning stalls**) and to **reinitialize**. However, reinitializing **layers/activations/callbacks/epochs mid-training** made the code overly complex. Instead, I adopted a simpler **restart policy** triggered by **high validation loss**, which works well in practice.

## Baseline model (with relu activation) trained with reinitialization

![Some models struggle to train initially due to the **dying ReLU** problem.](assets/images/base-mode-training-with-restart.png)

# Variance across runs and K-Fold cross-validation

::: {.slide style="font-size:70%"}
The **std** of the **minimum validation loss** was high (≈ half the mean). To obtain a more stable estimate:

- Used **K-Fold CV (K=10)** to report an average with variance.
- Sequential training became slow; tried multiprocessing with shared GPU but ran into errors.
- Switched to a clearer approach using **`subprocess.Popen`** to launch independent training processes, reschedule failures, and keep the parallelism logic outside the training code.
:::

# What to optimize next (and why)

::: {.slide style="font-size:70%"}
- **# encoder layers:** adding encoder depth doubles on-detector latency; avoid unless justified.
- **# decoder layers:** can be increased (offline) to improve reconstruction and better expose the bottleneck capacity.
- **Quantizing encoder weights/biases:** evaluate experimentally (likely symmetric per-channel weights; higher-precision biases).
- **Quantizing encoder output activation:** evaluate carefully (see §13).
- **Latent dimensionality:** we tested 2→5 for insight; **spec requires 2**, so we will stick to 2.
:::

# Parameter ranges and early experiments {.scrollable}

- **Weights/biases:** observed magnitudes are small; **integer bits 0–2** seem sufficient. With **10–15 fractional bits**, quantization error is well below the model’s validation loss per parameter.

---

![This chart shows the average data loss per weight or bias when applying PTQ, with our baseline model accuracy at approximately 0.0006](assets/images/basemodel-weight-bias-inspect.png)

---

- **Activations Outputs:** much **wider dynamic range**; they need more integer bits, making them harder to quantize aggressively.

---

- Quantizing **only the encoder output activation** gave inconsistent results (sensitive/random).

---

```{python}
import pickle
from typing import Dict, List, Tuple
import numpy as np

with open('assets/data/lr-2-latent-summary.pkl', 'rb') as handle:
    summary: Dict[Tuple[int, int, int], Tuple[List[float], List[float]]] = pickle.load(handle)


average_val_loss: Dict[Tuple[int, int], float] = {}
for bits in range(4, 21):
    for integer in range(0, bits // 2 + 1):
        average_val_loss[bits, integer] = 0.0
        for fold in range(1, 6):
            average_val_loss[bits, integer] += np.min(summary[(bits, integer, fold)][1])
        average_val_loss[bits, integer] /= 5

# Prepare data for 3D bar chart (move outside the loops)
bits_list = []
integer_list = []
avg_val_loss_list = []

for (bits_val, integer_val), avg_loss in average_val_loss.items():
    bits_list.append(bits_val)
    integer_list.append(integer_val)
    avg_val_loss_list.append(avg_loss)

import plotly.graph_objects as go

# Create 3D scatter plot with Plotly
fig_plotly = go.Figure(data=[go.Scatter3d(
    x=bits_list,
    y=integer_list,
    z=avg_val_loss_list,
    mode='markers',
    marker=dict(
        size=8,
        color=avg_val_loss_list,
        colorscale='Viridis',
        colorbar=dict(title="Average Validation Loss"),
        opacity=0.8
    ),
    text=[f'Bits: {b}, Integer: {i}, Loss: {l:.6f}' for b, i, l in zip(bits_list, integer_list, avg_val_loss_list)],
    hovertemplate='%{text}<extra></extra>'
)])

fig_plotly.update_layout(
    title='Average Validation Loss by Bits and Integer Parameters',
    scene=dict(
        xaxis_title='Bits',
        yaxis_title='Integer',
        zaxis_title='Average Validation Loss'
    ),
    width=1000,
    height=700
)

fig_plotly.show()
```

---

- Keeping that activation **unquantized**, we scanned **latent sizes 2–5**: moving from **2 → 3** latents improved accuracy by ~4× with ~1.5× model/energy cost, but **spec confines us to 2 latents** (post-review), and gains from **3→4→5** were not compelling.

---

```{python}
import pickle
from typing import Dict, List, Tuple
import numpy as np

def get_summary(
    path: str
) -> Dict[Tuple[int, int, int], Tuple[List[float], List[float]]]:
    with open(path, 'rb') as handle:
        summary: Dict[Tuple[int, int, int], Tuple[List[float], List[float]]] = pickle.load(handle)
    return summary

from plotly.subplots import make_subplots
import plotly.graph_objects as go

def draw_plots(summary, latent_dim):

    avg_min_val_losses = {}
    std_min_val_losses = {}
    
    avg_min_losses = {}
    std_min_losses = {}
    
    bits_lists = {}

    based_model_val_loss_list = []
    based_model_loss_list = []

    min_val_losses = []
    min_losses = []

    for bits, integer, fold in sorted(summary, key=lambda x: (x[1], x[0], x[2])):
        min_val_loss = np.min(summary[(bits, integer, fold)]['val_loss'])
        min_loss = np.min(summary[(bits, integer, fold)]['loss'])
        if bits == 0:
            based_model_val_loss_list.append(min_val_loss)
            based_model_loss_list.append(min_loss)
            continue
        
        if fold == 1:
            min_val_losses = [min_val_loss]
            min_losses = [min_loss]
        min_val_losses.append(min_val_loss)
        min_losses.append(min_loss)
        if fold == 10:
            if integer not in avg_min_val_losses:
                avg_min_val_losses[integer] = []
                std_min_val_losses[integer] = []
                avg_min_losses[integer] = []
                std_min_losses[integer] = []
                bits_lists[integer] = []
            avg_min_val_losses[integer].append(np.mean(min_val_losses))
            std_min_val_losses[integer].append(np.std(min_val_losses))
            avg_min_losses[integer].append(np.mean(min_losses))
            std_min_losses[integer].append(np.std(min_losses))
            bits_lists[integer].append(bits)

    base_avg_val_loss = np.mean(based_model_val_loss_list)
    base_std_val_loss = np.std(based_model_val_loss_list)
    base_avg_loss = np.mean(based_model_loss_list)
    base_std_loss = np.std(based_model_loss_list)
    
    
    # Plot with Plotly (2x3 grid): top row = validation, bottom row = training
    sorted_integers = sorted(bits_lists.keys())
    n = min(3, len(sorted_integers))
    titles = [f'integer = {k}' for k in sorted_integers[:n]] + [""] * (3 - n) + [""] * 3

    fig = make_subplots(rows=2, cols=3, subplot_titles=titles)

    for idx in range(n):
        integer_key = sorted_integers[idx]
        col = idx + 1

        x = np.array(bits_lists[integer_key]).astype(int)

        # Row 1: validation
        y_val = avg_min_val_losses[integer_key]
        yerr_val = std_min_val_losses[integer_key]
        fig.add_trace(
            go.Scatter(
                x=x, y=y_val, mode='lines+markers', name='Avg Min Val Loss',
                error_y=dict(type='data', array=yerr_val, visible=True),
                line=dict(color='orange'),
                showlegend=(idx == 0)
            ),
            row=1, col=col
        )
        # Base band (val)
        x_poly = list(x) + list(x[::-1])
        y_poly = ([base_avg_val_loss - base_std_val_loss] * len(x) +
                  [base_avg_val_loss + base_std_val_loss] * len(x))
        fig.add_trace(
            go.Scatter(
                x=x_poly, y=y_poly, fill='toself',
                fillcolor='rgba(255,0,0,0.2)', line=dict(color='rgba(0,0,0,0)'),
                name='Base Avg ±1σ (val)', showlegend=(idx == 0)
            ),
            row=1, col=col
        )
        fig.add_trace(
            go.Scatter(
                x=x, y=[base_avg_val_loss] * len(x), mode='lines',
                line=dict(color='red', dash='dash'),
                name=f'Base Avg (val) = {base_avg_val_loss:.5f}',
                showlegend=(idx == 0)
            ),
            row=1, col=col
        )

        # Row 2: training
        y_train = avg_min_losses[integer_key]
        yerr_train = std_min_losses[integer_key]
        fig.add_trace(
            go.Scatter(
                x=x, y=y_train, mode='lines+markers', name='Avg Min Loss',
                error_y=dict(type='data', array=yerr_train, visible=True),
                line=dict(color='blue'),
                showlegend=(idx == 0)
            ),
            row=2, col=col
        )
        # Base band (train)
        y_poly = ([base_avg_loss - base_std_loss] * len(x) +
                  [base_avg_loss + base_std_loss] * len(x))
        fig.add_trace(
            go.Scatter(
                x=x_poly, y=y_poly, fill='toself',
                fillcolor='rgba(255,0,0,0.2)', line=dict(color='rgba(0,0,0,0)'),
                name='Base Avg ±1σ (train)', showlegend=(idx == 0)
            ),
            row=2, col=col
        )
        fig.add_trace(
            go.Scatter(
                x=x, y=[base_avg_loss] * len(x), mode='lines',
                line=dict(color='red', dash='dash'),
                name=f'Base Avg (train) = {base_avg_loss:.5f}',
                showlegend=(idx == 0)
            ),
            row=2, col=col
        )

    # Axes titles and grid
    for c in range(1, 4):
        fig.update_xaxes(title_text='bits', showgrid=True, row=1, col=c)
        fig.update_xaxes(title_text='bits', showgrid=True, row=2, col=c)
        fig.update_yaxes(showgrid=True, row=1, col=c)
        fig.update_yaxes(showgrid=True, row=2, col=c)

    fig.update_yaxes(title_text=f'average min val loss over 10 folds (latent dim = {latent_dim})', row=1, col=1)
    fig.update_yaxes(title_text=f'average min loss over 10 folds (latent dim = {latent_dim})', row=2, col=1)

    fig.update_layout(height=700, width=1000, showlegend=True)
    fig.show()
```

```{python}
summary = get_summary('assets/data/2-latent-parallel-quantized-summary.pkl')
sorted(summary.keys())
draw_plots(summary, latent_dim=2)
```

---

```{python}
summary = get_summary('assets/data/3-latent-parallel-quantized-summary.pkl')
sorted(summary.keys())
draw_plots(summary, latent_dim=3)
```

---

```{python}
summary = get_summary('assets/data/4-latent-parallel-quantized-summary.pkl')
sorted(summary.keys())
draw_plots(summary, latent_dim=4)
```

---

```{python}
summary = get_summary('assets/data/5-latent-parallel-quantized-summary.pkl')
sorted(summary.keys())
draw_plots(summary, latent_dim=5)
```

---

Across runs, **integer-bit = 0** (i.e., all fractional) often performed best for weights/biases, matching the observed small dynamic range.

---

**Key observation:** Although run-to-run variance is high, **train and validation losses closely track**, especially at the end. The decoder has ~**2 KB** of parameters vs. ~**100 MB** of data, so the model capacity is too small to overfit; *low MSE loss* typically reflects genuine difficulty, not overfitting.

## 6 Random Models (bits=4)

```{python}
import pickle
from typing import Dict, List, Tuple
import numpy as np
import random
import plotly.graph_objects as go
from plotly.subplots import make_subplots


def get_summary(
    path: str
) -> Dict[Tuple[int, int, int], Tuple[List[float], List[float]]]:
    with open(path, 'rb') as handle:
        summary: Dict[Tuple[int, int, int], Tuple[List[float], List[float]]] = pickle.load(handle)
    return summary

summary = get_summary("assets/data/2-latent-parallel-quantized-summary.pkl")

# Select 6 random models from the summary
random.seed(39)  # For reproducibility

# Get all keys except the base model (bits=0)
available_keys = [key for key in summary.keys() if key[0] == 4]
selected_keys = random.sample(available_keys, 6)

# Create subplots
fig = make_subplots(
    rows=2, cols=3,
    subplot_titles=[f"(bits={key[0]}, integer={key[1]}, fold={key[2]})" for key in selected_keys]
    
)

for i, key in enumerate(selected_keys):
    bits, integer, fold = key
    loss_data = np.array(summary[key]['loss'], dtype=float)
    val_loss_data = np.array(summary[key]['val_loss'], dtype=float)
    epochs_data = np.arange(1, len(loss_data) + 1)
    
    row = i % 2 + 1
    col = i // 2 + 1
    
    # Add training loss
    fig.add_trace(
        go.Scatter(x=epochs_data, y=loss_data, mode='lines', name='train', 
                  line=dict(color='blue'), showlegend=(i==0)),
        row=row, col=col
    )
    
    # Add validation loss
    fig.add_trace(
        go.Scatter(x=epochs_data, y=val_loss_data, mode='lines', name='val',
                  line=dict(color='orange'), showlegend=(i==0)),
        row=row, col=col
    )

# Update layout
fig.update_layout(
    # title_text="Loss vs. Epoch for 6 Random Models",
    showlegend=True
)

# Update y-axes to log scale
fig.update_yaxes(type="log", title_text="Loss (MSE, log)")
fig.update_xaxes(title_text="Epoch")

fig.show()
```

---

**Implication for production:** choose a reasonable configuration and **rerun with new initializations** until the desired metric is achieved. Under this regime, we can also quantize the encoder activation more aggressively.

# Recommendations to the new QKeras replacing of CERN - HGQ2 (developer feedback)

::: {.slide style="font-size:70%"}
- Add **LeakyReLU-family** activations support; they help both small and large models and drastically reduce dead-unit risk.
- Prefer **explicit** over implicit implementation behavior and make implicit functionalities like auto quantization one top of them.
- Move away from **string-based** quantizer specs; adopt Pythonic APIs (with deprecation warnings).
- Consider **drop-in PyTorch-layer equivalents** for broader adoption.
:::

# Final reconstructions from test data

![**Alt+click** to zoom in on elements (**CTRL+click** in Linux)](assets/images/test-results.png)

# Summary

::: {.slide style="font-size:70%"}
- I worked on optimizing an autoencoder to compress pulses measured in the PicoCal calorimeter.
- I used mixed-precision quantization (**QKeras + hls4ml**) to quantize the encoder while maintaining accuracy.
- To avoid dead units, I used **Leaky ReLU** during development and a restart policy for the final product.
- To provide more meaningful results despite high variance, I used **K-Fold** training.
- I showed that accuracy can improve by up to **4×** by adding one latent variable.
- I demonstrated that we can reduce the encoder size by at least **4×** through quantization with no noticeable loss in accuracy, and potentially even more for the final product.
:::

# References
::: {#refs}
:::

# Send Regards 

This was my first time in Europe, and I absolutely loved the experience. My time at CERN was truly exceptional—the collaborative environment and welcoming atmosphere made it unforgettable.

I'm deeply grateful to everyone who makes this summer program possible. The impact you're creating by providing these opportunities to students is remarkable, and I feel fortunate to have been part of it.